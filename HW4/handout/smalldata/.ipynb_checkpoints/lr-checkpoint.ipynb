{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "with open('formatted_train.tsv','r') as f:\n",
    "    reader = csv.reader(f,delimiter = '\\t')\n",
    "    content = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39176"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = dict()\n",
    "with open('dict.txt','r') as f:\n",
    "    for line in f.readlines():\n",
    "        [word, num_str] = line.split()\n",
    "        word_dict[word] = int( num_str )\n",
    "\n",
    "dim = len(word_dict)\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W \n",
    "W = [0]*dim + [0]    # dim + 1 for the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = []\n",
    "feature_train = []\n",
    "\n",
    "num_feature = len(content)\n",
    "\n",
    "for line in content:\n",
    "    label_train.append( int(line[0]) )\n",
    "    comment = line[1:]\n",
    "    feature_dict = dict()\n",
    "    for ele in comment:\n",
    "        [key,val] = ele.split(':')\n",
    "        feature_dict[ int(key) ] = int(val)\n",
    "    feature_dict[ dim ] = 1        # bias term in the end\n",
    "    feature_train.append( feature_dict )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dot(X,W):\n",
    "    product = 0.0\n",
    "    for key in X.keys():\n",
    "        product += X[key]*W[key]\n",
    "    return product\n",
    "\n",
    "def cal_loss_i(theta,x,y):\n",
    "    loss_i = -sparse_dot(x, theta)*float(y) + math.log(  1 + math.exp(sparse_dot(x, theta))  )\n",
    "    return loss_i\n",
    "\n",
    "def cal_loss(feature,label,W):\n",
    "    \n",
    "    num_data = len(feature)\n",
    "    loss = 0.0\n",
    "    \n",
    "    for i in range(num_data):\n",
    "        loss += cal_loss_i(W, feature[i], label[i])\n",
    "    return loss\n",
    "\n",
    "def cal_gradient(theta,x,y):\n",
    "    gradient = {}\n",
    "    exp_term = math.exp( sparse_dot(x,theta) )\n",
    "    for key in x.keys():\n",
    "        \n",
    "        gradient[key] = -x[key]*(y - exp_term/( 1 + exp_term ) )\n",
    "    return gradient   \n",
    "\n",
    "def update(W,gradient,learning_rate):\n",
    "    \n",
    "    for key in gradient.keys():\n",
    "        W[key] -= learning_rate*gradient[key]\n",
    "    return W\n",
    "\n",
    "def train(feature,label,W,num_epoch, learning_rate):\n",
    "    \n",
    "    num_data = len(feature)\n",
    "    loss = []\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        seq_epoch = np.arange(num_data)\n",
    "        random.shuffle(seq_epoch)\n",
    "        loss.append( cal_loss(feature,label,W) )\n",
    "        for epoch in seq_epoch:\n",
    "            gradient = cal_gradient( W, feature[epoch], label[epoch])\n",
    "            W = update(W,gradient,learning_rate)\n",
    "            \n",
    "        \n",
    "    return loss\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69.31471805599459,\n",
       " 65.60096839542086,\n",
       " 1.22658368053563,\n",
       " 0.21746094533933047,\n",
       " 0.1648003689501009,\n",
       " 0.13921169191519928,\n",
       " 0.12174987258242352,\n",
       " 0.10883260052605662,\n",
       " 0.098754591186915,\n",
       " 0.09063190250594039,\n",
       " 0.08389944807943109,\n",
       " 0.07820719945145117,\n",
       " 0.07332752423312809,\n",
       " 0.06908822688818253,\n",
       " 0.06536584252456691,\n",
       " 0.06205078514043322,\n",
       " 0.05909498796341051,\n",
       " 0.05643904364781495,\n",
       " 0.05403444074435937,\n",
       " 0.0518431486818573,\n",
       " 0.04984029073176899,\n",
       " 0.047998923208343806,\n",
       " 0.04630199027742845,\n",
       " 0.0447305810225166,\n",
       " 0.04327163286556911,\n",
       " 0.04191327387349355,\n",
       " 0.04064488139199807,\n",
       " 0.03945689574648521,\n",
       " 0.038342528512167066,\n",
       " 0.03729490322400784,\n",
       " 0.036307024469008256,\n",
       " 0.035374025086895855,\n",
       " 0.03449185710354277,\n",
       " 0.033656160795302856,\n",
       " 0.032862832236373984,\n",
       " 0.032109138963451626,\n",
       " 0.03139200275404292,\n",
       " 0.030708090143230317,\n",
       " 0.030056066356957185,\n",
       " 0.029433132655375623,\n",
       " 0.028837344074368602,\n",
       " 0.028267160836230765,\n",
       " 0.02772048753298678,\n",
       " 0.027196069569026635,\n",
       " 0.026692600516005766,\n",
       " 0.02620890890710325,\n",
       " 0.025743582109455897,\n",
       " 0.025295720927608602,\n",
       " 0.02486419426292355,\n",
       " 0.024448292702989226,\n",
       " 0.024047025339602738,\n",
       " 0.023659505310269544,\n",
       " 0.023285093533462514,\n",
       " 0.0229232307036627,\n",
       " 0.022573169366504157,\n",
       " 0.02223438208310157,\n",
       " 0.02190626001019789,\n",
       " 0.02158845366477918,\n",
       " 0.02128031939380717,\n",
       " 0.02098147750249754,\n",
       " 0.02069142313757412,\n",
       " 0.02040974576911616,\n",
       " 0.02013618851366598,\n",
       " 0.01987038648098575,\n",
       " 0.019611913953207313,\n",
       " 0.019360553339739215,\n",
       " 0.019116016391084838,\n",
       " 0.018877907969368165,\n",
       " 0.01864600457977495,\n",
       " 0.018420155220206604,\n",
       " 0.018200029119318617,\n",
       " 0.017985462246655308,\n",
       " 0.017776232087874252,\n",
       " 0.017572133095401645,\n",
       " 0.017372923020684054,\n",
       " 0.01717848005498346,\n",
       " 0.01698861189468754,\n",
       " 0.016803155639042857,\n",
       " 0.016621958655457833,\n",
       " 0.016444880802457547,\n",
       " 0.01627175769844792,\n",
       " 0.016102479648443684,\n",
       " 0.015936906656979666,\n",
       " 0.015774905822096202,\n",
       " 0.015616381884820655,\n",
       " 0.015461193223372407,\n",
       " 0.015309258579062833,\n",
       " 0.015160502386039379,\n",
       " 0.015014738719270008,\n",
       " 0.014871927992405375,\n",
       " 0.014731966812470243,\n",
       " 0.01459479997651385,\n",
       " 0.014460306116256993,\n",
       " 0.0143284133131095,\n",
       " 0.014199052701636475,\n",
       " 0.014072135621499341,\n",
       " 0.013947612793126774,\n",
       " 0.013825398341624098,\n",
       " 0.013705445453942373,\n",
       " 0.013587669963626777]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(feature_train,label_train,W, 60, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('formatted_test.tsv','r') as f:\n",
    "    reader = csv.reader(f,delimiter = '\\t')\n",
    "    content = list(reader)\n",
    "\n",
    "label_test = []\n",
    "feature_test = []\n",
    "\n",
    "num_feature = len(content)\n",
    "\n",
    "for line in content:\n",
    "    label_test.append( int(line[0]) )\n",
    "    comment = line[1:]\n",
    "    feature_dict = dict()\n",
    "    for ele in comment:\n",
    "        [key,val] = ele.split(':')\n",
    "        feature_dict[ int(key) ] = int(val)\n",
    "    feature_dict[ dim ] = 1        # bias term in the end\n",
    "    feature_test.append( feature_dict )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(feature,label,W):\n",
    "    num_data = len(feature)\n",
    "    pred_label = []\n",
    "    error_num = 0\n",
    "    \n",
    "    for i in range(num_data):\n",
    "        x = feature[i]\n",
    "        #y = 1/( 1 + math.exp(-sparse_dot(x,W)) )\n",
    "        y = math.exp( sparse_dot(x,W) )/( 1 + math.exp( sparse_dot(x,W) ) )\n",
    "        if y >= 0.5:\n",
    "            pred_label.append(1)\n",
    "        else:\n",
    "            pred_label.append(0)\n",
    "            \n",
    "    for y1,y2 in zip(pred_label,label):\n",
    "        if y1 != y2:\n",
    "            error_num += 1\n",
    "    \n",
    "    accuracy = error_num/num_data\n",
    "    \n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(feature_test,label_test,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(feature_train,label_train,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
